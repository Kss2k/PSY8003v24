---
title: "Dummy_Moderation_and_Mediation"
output: html_notebook
---

```{r}
library(pprint)
library(haven)
library(Statamarkdown)
library(dplyr)
```

## Section 1: Dummy Variable Regression

```{r}
setwd("C:/Users/slupp/OneDrive/Skrivebord/NTNU/Mehmet/PSY8003/Dummy_moderation_and_mediation")
flats <- readRDS("flats.rds")

```

```{stata}
cd "C:\Users\slupp\OneDrive\Skrivebord\NTNU\Mehmet\PSY8003\Dummy_moderation_and_mediation"
use flats.dta
```

### Comparing Means

1.  If we're only using a single categorical variable in our regression it is akin to a t-test or a one-way ANOVA
2.  While we can create dummy variables in our dataset, both STATA and R have built in ways of handling categorical variables their regression command/function.
3.  For this to work it is important that our variable is coded correctly (i.e., as a factor), and it should also have labeled values
4.  Lets say we want to compare mean-prices for flats at different locations

#### Labeling Values

**R**

```{r}
# NB the variable is already coded as a factor, so this is only to show how its done 
# SO DO NOT RUN/OVERWRITE ORIGINAL DATASET
flats2 <- flats %>% mutate(location = factor(location,
                                          levels = c(1, 2, 3, 4),
                                          labels = c("centre",
                                                     "south",
                                                     "west",
                                                     "east")))
```

**STATA**

```{stata}
// Creating variable with labels
label define labels_location 1 "centre" 2 "south" 3 "west" 4 "east"

// Applying Labels to location 
label values location labels_location 

codebook location
```

#### Regression: Comparing Means Location

If our variable is coded correctly (as a factor) we can see that R automatically treats it as a (or rather mutiple) dummy variable(s)

```{r}
reg_price_location <- lm(flat_price ~ location, data = flats)
preg(reg_price_location)
```

In STATA it is not that simple

```{stata}
reg flat_price location
```

In STATA we need to use a i. in front (Likewise we can force a categorical variable as continuous using c.)

```{stata}
reg flat_price i.location
```

This works even if our variable lacks labels

```{stata}
// removing labels from location
label values location .

//regression
reg flat_price i.location
```

#### **Does anyone want to interpret the results?**

1.  What is the mean flat price for each location (roughly)
2.  Are there any significant differences?

```{r}
cat(interpretation_price_location)
```

One problem with this approach is that we're only using planned contrasts. So we are unable to test the difference between all loactions. We could simply change the refrence group, and re-run the regression:

```{r}
flats2$location <- relevel(flats$location, ref = "south")

reg_price_location <- lm(flat_price ~ location, data = flats)
preg(reg_price_location)

```

In STATA we can simply use an extension of the i. operator: ib#, where \# indicates what variable to use as our base level (i.e., our reference group)

```{stata}
reg flat_price ib2.location
```

Of note we should adjust p-values when performing multiple comparisons, which there are many ways of doing. We will not look into it today.

### Boxplots

```{r}
library(ggplot2)
flats %>% ggplot(mapping = aes(x = location,
                               y = flat_price,
                               colour = location)) +
  geom_boxplot() +
  
  geom_jitter(size = 0.1) +
  
  stat_summary(fun = "mean", 
               geom = "point", 
               colour = "green") +
  
  stat_summary(fun.data = "mean_cl_boot", 
               geom = "errorbar",
               colour = "darkblue",
               width = 0.2) 
  


  
```

```{stata}
graph box flat_price, over(location)
```

![](images/Skjermbilde%202023-06-08%20130042.png){width="969"}

### Adding Covariates

1.  A covariate is essentially any variable which causes our coefficients to represent adjusted means, as opposed to actual means

Since (unless we add interaction terms) our models are additive, covariates can be both continous and categorical

#### Example of a Categorical Covariate

**R**

```{r}

# Renaming energy-efficiency to make the output a bit more readable
flats2 <- flats %>% rename(e_effic = energy_efficiency)

reg_price_location_energy <- lm(flat_price ~ location + e_effic,
                                data = flats2)
preg(reg_price_location_energy)
```

```{stata}
reg flat_price i.location i.energy_efficiency
```

#### Example of a Continous Covariate

**R**

```{r}
reg_price_location_floor_size <- lm(flat_price ~ location + floor_size,
                                    data = flats)
preg(reg_price_location_floor_size)
```

**STATA**

```{stata}
reg flat_price i.location floor_size
```

### Visualizing a dummy-variable regression with a continuous covariate

1.  A dummy-variable regression with a continiuous covariate can in essence be visualized as k (K = number of variable) different linear regressions with the same slope, where the dummy-variable coefficients represent differences in intercepts/constants (i.e., differences controlled for the variable on the x-axis)

**R**

```{r}
library(margins)
pred_price_location_size <- margins(reg_price_location_floor_size, 
        at = list(location = c("centre",
                               "south",
                               "west",
                               "east"),
                  floor_size = c(20,220)
        )
)

pred_price_location_size %>% ggplot(mapping = aes(x = floor_size,
                                                  y = fitted, 
                                                  colour = location)) +
  geom_smooth(method = "lm", se = F)
```

**STATA**

```{stata}
reg flat_price i.location floor_size
margins , at(floor_size = (20(50)220) location = (1(1)4))
marginsplot
```

![](images/Skjermbilde%202023-06-08%20141238.png){width="715"}

## Section 2: Interaction

1.  Interactionterms can be added directly in the regression command/function in both STATA and R
2.  In R we use the " \* " operator
    1.  This automatically adds the relevant variables as normal predictors as well
    2.  If we only want the interaction term (without the singular predictors) we can use the " : " operator
3.  In STATA " \* " = "\##", and " : " = "\#"

### Example of an interaction between two continuous variables

```{r}
reg_price_size_int_year <- lm(flat_price ~ floor_size * year_built, data = flats)
preg(reg_price_size_int_year)
```

**STATA**

In STATA we recommend that one uses c. or i. to specify, in some instances it can cause problems if they are not specified correctly (e.g., try this code `reg flat_price floor_size##year_built)`

```{stata}
reg flat_price c.floor_size##c.year_built
```

#### Visualizing interactions between two continuous variables

**3D**

```{r}
persp(reg_price_size_int_year, 
      xvar = "floor_size", 
      yvar = "year_built",
      theta = -30,
      phi = 15)
```

```{r}
library(plotly)

# Removing NA's 
flats_NA_removed <- na.omit(flats)

# Creating a grid representing our regression plane
grid_size <- 30
seq_floor_size <- seq(min(flats_NA_removed$floor_size, na.rm = T),
                      max(flats_NA_removed$floor_size, na.rm = T),
                      length.out = grid_size)

seq_year_built <- seq(min(flats_NA_removed$year_built, na.rm = T),
                      max(flats_NA_removed$year_built, na.rm = T),
                      length.out = grid_size)

year_size <- expand.grid(floor_size = seq_floor_size,
                   year_built = seq_year_built)

predicted_price <- matrix(predict(reg_price_size_int_year, 
                                  newdata = year_size),
                          ncol = grid_size, 
                          nrow = grid_size, 
                          byrow = TRUE) 


plot_ly(data = flats_NA_removed,
        type = "scatter3d",
        x = ~floor_size,
        y = ~year_built,
        z = ~flat_price,
        color = ~location,
        size = I(150)) %>%
  add_surface(x = seq_floor_size,
              y = seq_year_built,
              z = predicted_price,
              opacity = .3)

         
```

**2D**

```{r}
library(margins)

pred_price_size_int_year <- margins(reg_price_size_int_year,
                                    at = list(floor_size = c(20, 220),
                                              year_built = seq(1930, 2010, 20)
                                              )
                                    )
pred_price_size_int_year <- pred_price_size_int_year %>% 
  mutate(year_built = factor(year_built,
                             levels = seq(1930, 2010, 20),
                             labels = as.character(
                               seq(1930, 2010, 20)
                               )
                             )
         )

pred_price_size_int_year %>% 
  ggplot(mapping = aes(x = floor_size,
                       y = fitted,
                       colour = year_built)) +
  geom_smooth(method = "lm", se = F)
```

**STATA**

```{stata}
reg flat_price c.floor_size##c.year_built 
margins , at (floor_size = (20(200)220) year_built = (1930(20)2010))
marginsplot

```

![](images/Skjermbilde%202023-06-08%20145309.png){width="709"}

### Example: Interaction between a continuous and a categorical variable

1.  Allowing for interactions between a continuous and categorical variable, can in essence be thought of as four separate regressions with their own intercept and slope.

2.  This is even true when adding another continuous and/or categorical variable, but in that case all interaction terms have to be saturated (which quickly becomes complicated).

3.  In this example we have a continuous variable X1 and two separate dummy variables X2 and X3: $Y_i = \beta_0 + \beta_1X_{1i} + \beta_2 X_{2i} + \beta_3 X_{3i} + \beta_4 X_{1i} X_{2i} + \beta_5 X_{1i} X_{3i} + \beta_6 X_{2i} + \beta_7 X_{1i} X_{2i} X_{3i}$

    NB: We could simply use lm(`Y ~ X1*X2*X3, data = df)` in R, and `reg Y X1##X2##X3` in STATA

4.  This would then be the same as running a separate regression for each unique group (i.e., each unique combination of B2 and B3).

**R**

```{r}
reg_price_location_int_size <- lm(flat_price ~ location*floor_size, data = flats)
preg(reg_price_location_int_size)
```

**STATA**

```{stata}
reg flat_price i.location##c.floor_size 
```

1.  When interpreting this regression, the intercept of each location is the intercept (B0) + the coefficent for the location.
2.  The slope for floor_size for each location is the slope for floor_size + the slope for location
    1.  This is because the interaction term represents the change in slope per 1 unit change
    2.  And dummy variables are either 0 or 1.
3.  We can for example see that the highest intercept (i.e., predicted flat_price at floor_size = 0) belongs to centre (161 757), while the lowest belongs to west (161 757 - 288 110 = -126353).
4.  We can also see that the largest slope belongs to west (5347 + 2591 = 7938), while the smallest belongs to south (5347 - 1750 = 3597)

#### **Visualizing interaction between a categorical and a continuous variable**

**R: The Model**

```{r}
pred_location_int_floor <- margins(reg_price_location_int_size,
                                   at = list(location = c("centre",
                                                          "south",
                                                          "west",
                                                          "east"),
                                             floor_size = c(20,220)
                                             )
                                   )

pred_location_int_floor %>% ggplot(mapping = aes(x = floor_size,
                                                 y = fitted,
                                                 colour = location,
                                                 )
                                   ) +
  geom_smooth(method = "lm", se = F) 
  
                                   
                  
```

**R: Running separate regressions for each location**

Here you can see we get the same plot, showing that it indeed is equivalent to running four separate regressions (in terms of your coefficients).

```{r}
flats %>% ggplot(mapping = aes(x = floor_size,
                               y = flat_price,
                               colour = location)) +
  geom_smooth(method = "lm", se = FALSE, fullrange = T)
```

**STATA**

reg flat_price i.location##c.floor_size

margins , at(floor_size = (20(200)220) location = (1(1)4))

marginsplot

```{stata}
reg flat_price i.location##c.floor_size

margins , at(floor_size = (20(200)220) location = (1(1)4))
marginsplot
```

![](images/Skjermbilde%202023-06-09%20124142.png){width="665"}

### Exercises

1.  Run a regression with flat price as a dependent variable and energy efficiency as categorical variable

    1.  Optional: run the same regression by creating the dummy variables before hand, and by using the built in dummy-variable functionality in the lm() and reg command.

2.  Change the reference group in the regression model

3.  Add a continuous covariate to the regression model

4.  Visualize the regression in a two-dimensional plot.

5.  Run a multiple regression with an interaction between two continuous variables

6.  Interpret the results

7.  Visualize your model in 2D

8.  Run a multiple regression with an interaction between a continuous and a categorical variable

9.  Interpret the results

10. Visualize your model in 2D

## Section 3: Mediation Analysis

1.  In this section we're going to assess mediation based on Baron and Kenny's (1968)
2.  First we're going to go through the original regression approach
3.  Then we're going to show the adjusted SEM-approach

We're going to be using the *workou*t dataset

#### R

```{r}
setwd("C:/Users/slupp/OneDrive/Skrivebord/NTNU/Mehmet/PSY8003/Dummy_moderation_and_mediation")
workout <- readRDS("workout.rds")
View(workout)
```

**STATA**

```{stata}
// NB change profile.do
use workout.dta, clear
```

## Regression Approach: Baron and Kenny (1968)

1.  In a mediation analysis we have an independent variable $X$

2.  A mediating variable $M$

3.  An a dependent variable $Y$

4.  In mediation we have the direct effects of $X\mapsto M$, $M\mapsto Y$ and the direct relationship $c$ where $X \mapsto Y$

5.  Lastly we have the indirect relationship $c'$ where $X \mapsto Y$, while controlling for $M$

6.  These relationships are assessed through four steps

    **NB:** Note that the language implies causation, but that it really requires experimental data to be sure

### Steps

1.  Simple regressions assessing $c$, $X \mapsto Y$:

    $Y = \beta_0 + \beta_1 X_{i} + e_i$

2.  Simple regression assessing $X\mapsto M$ (effect *a*):

    $M = \beta_0 + \beta_1X_{i} +e_i$

3.  Simple regression assessing $M\mapsto Y$ (effect *b*):

    $Y = \beta_0 + \beta_1 X_{i} + e_i$

4.  Multiple regression assessing $c'$:

    $Y = \beta_0 + \beta_1 X_{i} + \beta_2 M_{i} + e_i$

### Interpreting Steps

1.  For there to be mediation all direct relationships (steps 1-3) must be significant
2.  If one (or multiple) of these are non-significant we usually assume there is no mediating relationship
3.  If steps 1-3 are significant, we proceed to step 4
4.  For there to be mediation, $M$ still needs to have a significant effect upon $Y$, when controlling for $X$
5.  If $X$ still has an effect upon $Y$ (i.e., it is significant) when controlled for $M$ it suggests partial mediation
6.  If $X$ no longer has an effect upon $Y$ (i.e., it is non-significant) when controlled for $M$ it suggests full mediation

### Calculating indirect effect

**Delta (difference in** $\beta_1$ **between step 1 and 4)**

$$
\beta_{indirect} =  \beta_{step_1} - \beta_{step4}
$$

Sobel product (product of $\beta$ from $X\mapsto M$ and $M\mapsto Y$ **)**

(i.e., this is the same approach as calculating a correlation matrix/covariance matrix from path analysis, where we multiply the effect of $X$ on $M$, with the partial effect of $M$ on $Y$)

$$
\beta_{indirect} = \beta_{X\mapsto M} - \beta_{partialM\mapsto Y}
$$

#### Conveniently Sobel = Delta

### In R:

Y = calories

X = attract

M = appear

**Step 1**

```{r}
step_1 <- lm(calories ~ attract, data = workout)
preg(step_1)
```

```{stata}
reg calories attract
```

We can see that the effect of appear on calories is significant

**Step 2**

```{r}
step_2 <- lm(appear ~ attract, data = workout)
preg(step_2)

```

```{stata}
reg appear attract
```

We also see that the effect of attract on appear is significant

**Step 3**

```{r}
step_3 <- lm(calories ~ appear, data = workout)
preg(step_3)
```

```{stata}
reg calories appear
```

Since appear also has a significant effect upon appear we can move on to step 4

**Step 4**

```{r}
step_4 <- lm(calories ~ attract + appear, data = workout)
preg(step_4)
```

```{stata}
reg calories attract appear
```

M is significant which points towards mediation. Since attract has become non-significant it suggests full mediation.

### Indirect effects (this is only to show the computation)

NB: There is a popular package for this in R, but it requires an older version of R

**Delta**

```{r}
delta <- step_1$coefficients[2] - step_4$coefficients[2]
delta
```

**Sobel**

```{r}
alpha <- step_2$coefficients[2] 
beta <- step_4$coefficients[3]
sobel <- alpha * beta
sobel

```

We can use sobels effect to calculate its std.error, which can give us a p-statistic

```{r}
std.error_alpha <- summary(step_2)[["coefficients"]][2,2]
std.error_beta <- summary(step_4)[["coefficients"]][3,2]

sobel_z <- sqrt(alpha^2*std.error_beta^2 + 
                  beta^2*std.error_alpha^2 +
                  std.error_alpha^2*std.error_beta^2) 
sobel_z

CI_low = sobel - sobel_z*1.96
CI_high = sobel + sobel_z*1.96
cat("CI_low =", CI_low, "CI_high =", CI_high)
```

## SEM Approach

1.  There are multiple advantages of using the SEM-approach, as opposed to the regression approach
2.  When we only use observed variables we should get the same coefficents but we will get a bit larger std.errors (around \< .001 with N \> 200)
3.  The biggest advantage is that it is simpler, since you simply need a single model
4.  It also allows you to use latent variables, not only observed variables

```{r}
# install.packages("lavaan", dependencies = T)
# devtools::install_github("ihrke/rmedsem")

library(rmedsem)
library(lavaan)

```

```{stata}
// net install github, from("https://haghish.github.io/github/")
// github install mmoglu/medsem
```

### R: Lavaan Syntax

1.  **Predict: \~** (e.g., Y \~ X)

2.  **Indication: =\~** (Latent Variable \~ Meausured/Observed Variable

3.  \~\~ **covariance** (e.g., X \~\~ X)

4.  **\~1 intercept** or mean (e.g., X \~ 1 estimates the mean of variable X)

5.  **1\* fixes parameter/loading** to one (useful single item constructs)

6.  **NA\*** **frees** **parameter** or loading (useful to override default marker method, (e.g., f =\~ NA\*q)

7.  **a\*** **labels the parameter** 'a', used for model constraints (e.g., f =\~ a\*q)

```{r}

model <- "
    appear ~ attract
    calories ~ attract + appear
"
sem_1 <- sem(model, data = workout)
summary(sem_1)

```

### **STATA: SEM syntax**

1.  The Syntax is quite similar in STATA, but its a bit simpler
2.  "+" is substiduted with " " ("spacebar") and both "=\~" and " \~" ar substituted with "-\>" ("\<-" also works)
3.  *medsem, indep(varname) med(varname) dep(varname) [mcreps(number) stand zlc rit rid]*

```{stata}
sem (calories <- attract appear) (appear <- attract)
sem, standardized
```

### Using the medsem package

#### Adjusted Baron and Kenny's approach (default)

1.  If both or one of the X-\>M and M-\>Y coefficients is not significant, there is no mediation

2.  When both of the X-\>M and M-\>Y coefficients are significant, there is "some" mediation

    1.  If the Sobel's z-test is significant and the X-\>Y coefficient is not significant, then there is complete mediation

    2.  If both the Sobel's z-test and the X-\>Y coefficients are significant, then there is partial mediation

    3.  If the Sobel's z-test is not significant but the X-\>Y coefficient is significant, then there is partial mediation

    4.  If neither Sobel's z-test nor the X-\>Y coefficient are significant, then there is partial mediation

```{r}
med_1 <- rmedsem(sem_1, 
        indep = "attract", 
        med = "appear", 
        dep = "calories",
        approach = c("bk", "zlc")) 
med_1
```

```{stata}
sem (calories <- attract appear) (appear <- attract) 
medsem, indep(attract) med(appear) dep(calories)


```

```{r}
#install.packages("tidySEM")
library(tidySEM)
graph_sem(msem_1)
```

### Exercises

1.  Use the regression approach to test the relationship between one independent, one mediating and one dependent variable
2.  Use the SEM approach to test the same model
3.  Interpret the results? (Is it no-, some- or full mediation?)
4.  Try to add some extra variables to your model using the SEM approach (tip: the SEM model is the only thing that changes, the medsem variables are the same)
5.  Try adding an extra mediating relationship in your model (tip: now you have to run two medsem commands, one to test each mediation)
6.  Try to create a SEM model with three latent variables: one independent, one mediating and one dependent latent variables
    1.  tip: use the =\~ operator in R,
    2.  In STATA use either -\> or \<-, but remember that observed variables (in traditional SEM), are functions of your latent variable not the other way around (i.e., it is akin to multiple simple regressions, not a single multiple regression).

## Appendix

### Appendix: Interpretation (only used to call via cat())

```{r echo=FALSE}
interpretation_price_location <- "
Mean Centre = 597 721$ \n
Mean South  = 425 470$ (p = .038) \n
Mean West   = 513 788$ \n
Mean East   = 499 728$ \n
We can also see that the model as a whole is non-significant with an R^2 around 3%

"
```
